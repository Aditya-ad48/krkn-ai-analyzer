# Krkn-AI Analyzer â€“ Architecture

## Overview

Krkn-AI Analyzer is an analysis and visualization layer built on top of
Krkn-AI chaos engineering results. While Krkn-AI focuses on *generating*
and *executing* chaos experiments, this project focuses on *interpreting*
their outcomes in a scalable, explainable, and operator-friendly way.

The system ingests native Krkn-AI result artifacts (JSON, CSV, YAML),
normalizes them into a canonical schema, and applies multi-agent analysis,
anomaly detection, historical comparison, and interactive visualization.

---

## Design Goals

1. **Schema-first design**  
   All inputs (synthetic or real) are normalized into a single
   `ExperimentResult` contract using Pydantic models.

2. **Separation of concerns**  
   - Krkn-AI â†’ experiment execution  
   - This project â†’ analysis, insight, visualization

3. **Explainability over black-box AI**  
   LLM output is always grounded in numeric evidence and file-level citations.
   Structured JSON output with Pydantic validation ensures consistency.

4. **Incremental adoption**  
   Works with partial data (e.g., no health checks, no Prometheus metrics).
   Graceful degradation when components are unavailable.

5. **Production-ready architecture**  
   Modular agents, extensible parsers, clear error handling.

---

## High-Level Architecture

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚    Krkn-AI Results        â”‚
        â”‚  (JSON / CSV / YAML)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
           â”‚ Auto-detect â”‚
           â”‚   Format    â”‚
           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  KrknResultsLoader        â”‚
        â”‚  - ScenarioParser         â”‚
        â”‚  - HealthParser           â”‚
        â”‚  - FitnessParser          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Canonical Schema        â”‚
        â”‚   ExperimentResult        â”‚
        â”‚   (Pydantic Models)       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚        Orchestrator            â”‚
    â”‚  (multi-agent coordinator)    â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
        â”‚       â”‚        â”‚      â”‚
 â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â” â”Œâ–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Fitness  â”‚ â”‚Health â”‚ â”‚ SLO â”‚ â”‚  Anomaly  â”‚
 â”‚  Agent   â”‚ â”‚ Agent â”‚ â”‚Agentâ”‚ â”‚ Detector  â”‚
 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
        â”‚         â”‚        â”‚          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Root Cause Agent        â”‚
        â”‚   (Groq LLM)              â”‚
        â”‚   - Structured output     â”‚
        â”‚   - Evidence citations    â”‚
        â”‚   - Ranked remediations   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Streamlit UI            â”‚
        â”‚   - Dashboard             â”‚
        â”‚   - AI Analysis           â”‚
        â”‚   - Comparison            â”‚
        â”‚   - Reports               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Components

### 1. Data Ingestion Layer

#### KrknResultsLoader
- **Purpose**: Auto-detect and load Krkn-AI experiment artifacts
- **Capabilities**:
  - Auto-detection of file formats (JSON, CSV, YAML)
  - Graceful handling of missing files
  - Raw file path tracking for provenance

#### Parsers
- **ScenarioParser**: Parses `best_scenarios.json` and per-generation YAML files
- **HealthParser**: Parses `health_check_report.csv` with NaN handling
- **FitnessParser**: Extracts fitness scores across generations

#### Schema (Pydantic Models)
```python
ExperimentResult
â”œâ”€â”€ metadata: ExperimentMetadata
â”œâ”€â”€ scenarios: List[Scenario]
â”œâ”€â”€ fitness: List[FitnessRecord]
â”œâ”€â”€ health_events: List[HealthEvent]
â”œâ”€â”€ prometheus_metrics: Optional[List[Dict]]
â””â”€â”€ raw_files: Dict[str, str]  # Provenance tracking
```

---

### 2. Multi-Agent Analysis Layer

| Agent | Responsibility | Key Outputs |
|------|----------------|-------------|
| **FitnessAgent** | Fitness evolution, convergence, plateaus | Best/avg/worst per generation, trend detection, slope analysis |
| **HealthAgent** | Failure correlation, MTTR, cascade hints | Failure counts by service, MTTR in seconds, cascade patterns |
| **SLOAgent** | Threshold validation, severity classification | Violations list, error rate, P99 latency, pass/fail status |
| **AnomalyDetector** | ML-based outlier detection | Fitness anomalies (Isolation Forest), cascade failures, slow recovery alerts |
| **RootCauseAgent** | LLM-assisted RCA with citations | Structured hypothesis, confidence score, evidence, remediations |

#### Agent Orchestration
The `Orchestrator` class:
- Coordinates sequential execution of agents
- Passes intermediate results between agents
- Handles agent failures gracefully
- Aggregates outputs into unified analysis dict

---

### 3. Root Cause Agent (LLM)

#### Design Philosophy
- **LLM as reasoning layer, not data source**
- **Structured output enforced via JSON schema**
- **Evidence must cite actual experiment files/lines**
- **Confidence scoring based on evidence quality**

#### Architecture
```python
RootCauseAgent
â”œâ”€â”€ __init__: Initialize Groq LLM client
â”œâ”€â”€ build_structured_prompt: Generate JSON schema-enforced prompt
â””â”€â”€ analyze: 
    â”œâ”€â”€ Check for scenarios & LLM availability
    â”œâ”€â”€ Build prompt with evidence
    â”œâ”€â”€ Invoke LLM with JSON response format
    â”œâ”€â”€ Parse & validate with Pydantic (StructuredRCA)
    â””â”€â”€ Return structured output or fallback
```

#### Output Schema (Pydantic)
```python
StructuredRCA
â”œâ”€â”€ hypothesis: str              # 1-2 sentence root cause
â”œâ”€â”€ confidence: float (0.0-1.0)  # Evidence-based confidence
â”œâ”€â”€ affected_components: List[str]
â”œâ”€â”€ evidence: List[EvidenceItem]
â”‚   â”œâ”€â”€ file: str               # Source file name
â”‚   â”œâ”€â”€ line: Optional[str]     # Line number/section
â”‚   â””â”€â”€ detail: str             # What this shows
â”œâ”€â”€ remediations: List[RemediationStep]
â”‚   â”œâ”€â”€ step: str               # Action item
â”‚   â”œâ”€â”€ impact: str             # high/medium/low
â”‚   â””â”€â”€ rationale: str          # Why this helps
â””â”€â”€ missing_data: Optional[List[str]]  # Observability gaps
```

#### Fallback Behavior
When LLM unavailable or scenarios missing:
- Deterministic analysis of health events
- Generic remediation recommendations
- Clear indication of fallback mode
- Suggestions to enable full functionality

---

### 4. Analytics Layer

#### Anomaly Detection (ML)
**Isolation Forest** for fitness anomaly detection:
- Features: [generation, fitness_score]
- Contamination: 0.15 (15% expected anomalies)
- Outputs: Anomaly indices, scores, generations

**Cascade Failure Detection**:
- Time-window correlation (30-second buckets)
- Concurrent failure identification
- Service correlation matrix

**Slow Recovery Detection**:
- Failure window tracking per service
- Threshold-based alerting (default: 60s)
- Severity classification (warning/critical)

---

### 5. Visualization Layer

#### Components
- **fitness_viz.py**: Best/avg/worst evolution charts
- **heatmap.py**: Service failure correlation matrices
- **network_graph.py**: Service dependency graphs with cascade edges

#### Technologies
- **Plotly**: Interactive charts with zoom/hover
- **NetworkX**: Graph layout algorithms
- **Pandas**: Data aggregation and transformation

---

### 6. User Interface (Streamlit)

#### Page Structure
```
Main Page (app/main.py)
â”œâ”€â”€ Experiment loader
â”œâ”€â”€ Auto-detect format
â””â”€â”€ Trigger analysis

Dashboard (1_ðŸ“Š_Dashboard.py)
â”œâ”€â”€ Experiment metadata
â”œâ”€â”€ Fitness evolution chart
â”œâ”€â”€ Health timeline
â”œâ”€â”€ Anomaly detection results
â””â”€â”€ Service dependency graph

AI Analysis (2_ðŸ¤–_AI_Analysis.py)
â”œâ”€â”€ Structured RCA display
â”œâ”€â”€ Evidence citations
â”œâ”€â”€ Ranked remediations
â”œâ”€â”€ Agent output tabs
â””â”€â”€ Export functionality

Comparison (3_ðŸ“ˆ_Comparison.py)
â”œâ”€â”€ Side-by-side metrics
â”œâ”€â”€ Dual fitness evolution
â”œâ”€â”€ Winner analysis
â””â”€â”€ Detailed comparison table

Reports (4_ðŸ“‹_Reports.py)
â””â”€â”€ JSON export with metadata
```

---

## Data Flow

### Analysis Workflow
```
1. User selects experiment directory
2. KrknResultsLoader auto-detects files
3. Parsers normalize to ExperimentResult
4. Orchestrator executes agents sequentially:
   a. FitnessAgent â†’ trend analysis
   b. HealthAgent â†’ MTTR & cascades
   c. SLOAgent â†’ violation detection
   d. AnomalyDetector â†’ ML outliers
   e. RootCauseAgent â†’ LLM reasoning
5. Results stored in st.session_state
6. UI pages render analysis
7. User exports JSON report
```

---

## Error Handling

### Graceful Degradation Strategy
- **Missing files**: Skip optional components (e.g., Prometheus)
- **NaN values**: Clean with pandas `isna()` checks
- **LLM failures**: Use deterministic fallback
- **Invalid JSON**: Return error with raw response excerpt
- **Import errors**: Conditionally import optional dependencies

### Validation
- Pydantic models validate all data at parse time
- Schema mismatches caught early with clear error messages
- File existence checks before parsing

---

## Extensibility

### Adding New Agents
1. Create agent class in `src/agents/`
2. Implement `analyze(exp: ExperimentResult) -> Dict[str, Any]`
3. Register in `Orchestrator.__init__`
4. Add to orchestrator workflow

### Adding New Parsers
1. Create parser class in `src/parsers/`
2. Implement `parse(file_path: Path) -> List[Model]`
3. Register in `KrknResultsLoader.load()`

### Adding New Visualizations
1. Create viz function in `src/visualizations/`
2. Return Plotly `go.Figure`
3. Call from Streamlit page with `st.plotly_chart()`

---

## Performance Considerations

### Optimization Strategies
- **Lazy loading**: Only parse files when needed
- **Caching**: Streamlit `@st.cache_data` for expensive operations
- **Batch processing**: Group similar operations (e.g., all parsers run together)
- **Async LLM calls**: Use `langchain` async for future parallelization

### Scalability
- Current design handles experiments with:
  - 100+ scenarios
  - 10,000+ health events
  - 10+ generations
- For larger datasets, consider:
  - Sampling strategies
  - Database backend (replace in-memory)
  - Distributed processing

---

## Security Considerations

- **API Keys**: Stored in `.env`, never committed to git
- **Input validation**: All user inputs validated via Pydantic
- **File path sanitization**: Pathlib prevents directory traversal
- **LLM prompt injection**: Structured JSON output limits injection vectors

---

## Testing Strategy

### Current Coverage
- Synthetic data for unit testing
- Manual testing with real Krkn-AI outputs
- Schema validation via Pydantic

### Future Testing
- Unit tests for each agent
- Integration tests for full workflow
- Property-based testing for parsers
- LLM output validation tests

---

## Deployment

### Local Development
```bash
streamlit run app/main.py
```

### Production Deployment
```bash
# Using Streamlit Cloud
streamlit run app/main.py --server.port 8501

# Using Docker
docker build -t krkn-analyzer .
docker run -p 8501:8501 krkn-analyzer
```

---

## Future Architecture Enhancements

### Planned Improvements
1. **LangGraph Integration**: Replace Orchestrator with LangGraph for:
   - Conditional agent execution
   - Parallel agent execution
   - Agent communication protocols

2. **RAG Enhancement**: Extend ChromaDB usage for:
   - Historical experiment search
   - Pattern recognition across experiments
   - Auto-remediation suggestions

3. **Real-time Analysis**: Stream processing for:
   - Live experiment monitoring
   - Progressive analysis updates
   - Alert generation

4. **Plugin System**:
   - Custom agent registration
   - Third-party visualization plugins
   - External integrations (Slack, PagerDuty)

---

## Non-Goals

- Executing chaos experiments (use Krkn-AI)
- Modifying Krkn-AI core logic
- Replacing observability platforms (Prometheus, Grafana)
- General-purpose chaos engineering framework

---

## Summary

Krkn-AI Analyzer acts as the **analysis and reasoning layer** for Krkn-AI,
bridging the gap between raw chaos output and actionable resilience insight.

The architecture prioritizes:
- **Explainability**: Every insight traceable to source data
- **Modularity**: Agents operate independently
- **Extensibility**: Easy to add new capabilities
- **Robustness**: Graceful handling of missing data/services
- **Production-readiness**: Clear error handling, validation, security

This design enables rapid iteration while maintaining a path to production deployment and CNCF-quality standards.
